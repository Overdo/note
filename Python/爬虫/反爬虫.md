### 传统反爬虫手段

- 1、后台对访问进行统计，如果单个IP访问超过阈值，予以封锁。
  - 这个虽然效果还不错，但是其实有两个缺陷，一个是非常容易误伤普通用户，另一个就是，IP其实不值钱，几十块钱甚至有可能买到几十万个IP。所以总体来说是比较亏的。
- 2、后台对访问进行统计，如果单个session访问超过阈值，予以封锁。
  - 这个看起来更高级了一些，但是其实效果更差，因为session完全不值钱，重新申请一个就可以了。
- 3、后台对访问进行统计，如果单个userAgent访问超过阈值，予以封锁。
  - 这个是大招，类似于抗生素之类的，效果出奇的好，但是杀伤力过大，误伤非常严重，使用的时候要非常小心。
- 4、以上的组合
  - 组合起来能力变大，误伤率下降，在遇到低级爬虫的时候，还是比较好用的。



### 真正实用的反爬虫手段

1. 通过更改连接地址，来让对方抓取到错误信息。这种方法简单，但是如果对方针对性的来查看，十分容易被发现。

2. 纯JAVASCRIPT反爬虫，更改key。这种做法简单，不容易被发现。但是可以通过有意爬取错误价格的方式来实现。

3. 纯JAVASCRIPT反爬虫，更改动态key。这种方法可以让更改key的代价变为0，因此代价更低。

4. 纯JAVASCRIPT反爬虫，十分复杂的更改key。这种方法，可以让对方很难分析，如果加了后续提到的浏览器检测，更难被爬取。

   ​

### 如何编写高级爬虫

##### **分布式**

通常会有一些教材告诉你，为了爬取效率，需要把爬虫分布式部署到多台机器上。这完全是骗人的。分布式唯一的作用是：防止对方封IP。封IP是终极手段，效果非常好，当然，误伤起用户也是非常爽的。

##### **模拟JavaScript**

有些教程会说，模拟javascript，抓取动态网页，是进阶技巧。但是其实这只是个很简单的功能。因为，如果对方没有反爬虫，你完全可以直接抓ajax本身，而无需关心js怎么处理的。如果对方有反爬虫，那么javascript必然十分复杂，重点在于分析，而不仅仅是简单的模拟。

换句话说：这应该是基本功。

**PhantomJs**

这个是一个极端的例子。这个东西本意是用来做自动测试的，结果因为效果很好，很多人拿来做爬虫。但是这个东西有个硬伤，就是：效率。此外PhantomJs也是可以被抓到的，出于多方面原因，这里暂时不讲。　

























































































